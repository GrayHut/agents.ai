{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efdab4ad",
   "metadata": {},
   "source": [
    ">RAG(Retrieval-Augmented Generation)\n",
    "\n",
    "- It's a framework that helps LLMs be more accurate, up-to-date & produce contexually relevant responses.\n",
    "- Retrieval: retrieve relevant information from a knowledge base or an external source, for example, using text embeddings stored in a vector store.\n",
    "- Generation: insert the relevant information to the prompt for the LLM to generate information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8039217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai import Mistral\n",
    "import requests\n",
    "import numpy as np\n",
    "import faiss        #pip install faiss-cpu for newer python versions - python 3.11, python 3.12\n",
    "import os\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "508be558",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "api_key=os.getenv(\"MISTRAL_KEY\")\n",
    "client=Mistral(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "250bfdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=requests.get('https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt')\n",
    "text=response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dded6858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split document into chunks\n",
    "#in a RAG system, it is crucial to split the document into smaller chunks so that \n",
    "# it's more effective to identify and retrieve the most relevant information in the retrieval process later\n",
    "\n",
    "chunk_size=2048\n",
    "chunks=[text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "len(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "798021e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating embeddings for each text chunk\n",
    "\n",
    "def get_text_embedding(input):\n",
    "    embeddings_batch_response=client.embeddings.create(\n",
    "        model=\"mistral-embed\",\n",
    "        inputs=input\n",
    "    )\n",
    "    return embeddings_batch_response.data[0].embedding\n",
    "    #text_embeddings=np.array(get_text_embedding(chunk) for chunk in chunks)\n",
    "\n",
    "\n",
    "if not chunks:\n",
    "    raise ValueError(\"Input 'chunks' is empty\")\n",
    "\n",
    "#collect embeddings into a list\n",
    "embeddings_list = [get_text_embedding(chunk) for chunk in chunks]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7360a49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_embeddings = np.array(text_embeddings)  # Convert to NumPy array\n",
    "# if text_embeddings.ndim == 1:  # If 1D, reshape to 2D\n",
    "#     text_embeddings = text_embeddings.reshape(1, -1)\n",
    "\n",
    "text_embeddings = np.array(embeddings_list, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9d304b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.03979492  0.07733154  0.00013709 ... -0.01274109 -0.02101135\n",
      "  -0.00264168]\n",
      " [-0.03152466  0.07226562  0.02961731 ... -0.01079559 -0.01189423\n",
      "  -0.00821686]\n",
      " [-0.05905151  0.06112671  0.01206207 ... -0.0226593   0.00488663\n",
      "  -0.00665283]\n",
      " ...\n",
      " [-0.05477905  0.06890869  0.02703857 ... -0.02456665 -0.02526855\n",
      "  -0.02687073]\n",
      " [-0.03884888  0.05587769  0.04718018 ... -0.01812744  0.00926208\n",
      "  -0.00866699]\n",
      " [-0.03048706  0.05831909  0.01704407 ... -0.01620483 -0.01800537\n",
      "  -0.04415894]]\n",
      "<class 'numpy.ndarray'>\n",
      "(37, 1024)\n"
     ]
    }
   ],
   "source": [
    "#loading into vector DB\n",
    "#Once we get the text embeddings, a common practice is to store them in a vector database for efficient processing and retrieval\n",
    "#When selecting a vector database, there are several factors to consider including speed, scalability,\n",
    "#cloud management, advanced filtering, and open-source vs. closed-source.\n",
    "print(text_embeddings)\n",
    "print(type(text_embeddings))\n",
    "print(text_embeddings.shape)\n",
    "d=text_embeddings.shape[1]\n",
    "index=faiss.IndexFlatL2(d)\n",
    "index.add(text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "162dc3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating embeddings for a question\n",
    "\n",
    "#question = \"What were the two main things the author worked on before college?\"\n",
    "#question=\"What is the meaning of the name Nicole\"\n",
    "question=\"Give me the general gist of the essay\"\n",
    "question_embeddings = np.array([get_text_embedding(question)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbbd4602",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We perform a search on the vector database with index.search, which takes two arguments: the first is the vector of the question embeddings, \n",
    "# and the second is the number of similar vectors to retrieve\n",
    "\n",
    "D, I = index.search(question_embeddings, k=2) # distance, index\n",
    "retrieved_chunk = [chunks[i] for i in I.tolist()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c948fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{retrieved_chunk}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: {question}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b01e979b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The essay discusses the author's realization of the power of publishing content online, particularly essays, and how this democratized access to an audience. The author recounts his experience of sharing a talk on Lisp programming, which unexpectedly gained significant attention after being posted on Slashdot. This event made him realize that the web allowed anyone to publish and reach a wide audience, bypassing traditional gatekeepers like editors and publishers.\\n\\nThe author reflects on the limitations of print media, which restricted the publication of essays to a select few, and highlights how the web opened up new opportunities for a broader range of writers. He sees this as a turning point in his career, deciding to continue writing essays online alongside other work.\\n\\nAdditionally, the author touches on his experiences with painting and working at a software company, highlighting lessons learned about attention to detail and the dynamics of software development. Overall, the essay emphasizes the transformative potential of online publishing and the personal insights gained through various professional experiences.\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_mistral(user_message, model=\"mistral-large-latest\"):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \"content\": user_message\n",
    "        }\n",
    "    ]\n",
    "    chat_response = client.chat.complete(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return (chat_response.choices[0].message.content)\n",
    "\n",
    "run_mistral(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
